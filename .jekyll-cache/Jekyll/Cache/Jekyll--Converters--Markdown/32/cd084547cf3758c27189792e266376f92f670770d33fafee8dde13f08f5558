I"V<p><img style="float: right;     margin-top: 25px;     margin-bottom: 35px;     max-width: 30%;     border: 6px solid #ddd;     border-radius: 50%;     box-sizing: border-box;" src="files/picture_1.jpg" /></p>

<p>I am a master’s student in Mathematics and Statistics at
<a href="https://www.mcgill.ca/">McGill University</a>,
supervised by <a href="http://www.math.mcgill.ca/dstephens/">David Stephens</a>.
I completed my bachelor’s degree in
<a href="https://www.mcgill.ca/mathstat/undergraduate/programs/b-sc/joint-honours-mathematics-and-computer-science-b-sc">Honours Mathematics and Computer Science</a>.</p>

<p>My research interests lie at the intersection of theory and algorithms
for machine learning and statistics. My recent focus
has been on developing new variance reduction methods 
based on importance sampling to accelerate the convergence
of optimization and sampling algorithms for large finite-sum problems.</p>

<p>More broadly, I am interested in developing the computational<br />
and statistical tools needed to build systems that can make optimal<br />
decisions under uncertainty.</p>

<p><a href="files/resume.pdf">Resume</a> / <a href="https://scholar.google.com/citations?user=5ZzcGmgAAAAJ&amp;hl=en&amp;oi=ao">Google Scholar</a></p>

<h3 id="papers">Papers</h3>
<p><strong>Adaptive Importance Sampling for Finite-Sum
Optimization and Sampling with Decreasing Step-Sizes.</strong><br />
Ayoub El Hanchi, David A. Stephens<br />
<em>Advances in Neural Information Processing Systems, 2020</em><br />
<a href="files/paper_1.pdf">paper</a> | <a href="files/presentation_1.pdf">slides</a> | <a href="files/poster_1.pdf">poster</a></p>

<h3 id="preprints">Preprints</h3>
<p><strong>Stochastic Reweighted Gradient Descent.</strong><br />
Ayoub El Hanchi, David A. Stephens<br />
<a href="files/paper_3.pdf">paper</a></p>

<p><strong>A Lyapunov Analysis of Loopless SARAH.</strong><br />
Ayoub El Hanchi<br />
<a href="files/paper_2.pdf">paper</a></p>

<h3 id="thesis">Thesis</h3>
<p><strong>Large Scale Optimization and Sampling for Machine Learning and Statistics. (in progress)</strong><br />
Ayoub El Hanchi<br />
<a href="files/thesis_1.pdf">draft</a></p>

<h3 id="software">Software</h3>
<p><strong>TorchVr (in progress)</strong><br />
A PyTorch library providing PyTorch modules and samplers that produce efficient gradient estimators to accelerate training of large scale models.<br />
All samplers are written in C++ using an efficient tree implementation for increased performance.
The C++ code is then exposed to python
using <a href="https://github.com/pybind/pybind11">pybind11</a>.<br />
<a href="files/code_1.zip">source</a></p>

<h3 id="old-reports">Old Reports</h3>
<ul>
  <li><em>Langevin Diffusion as Gradient Flow in Wasserstein Space.</em><br />
<a href="files/report_4.pdf">report</a></li>
  <li><em>Scaling up MCMC for Bayesian inference using adaptive data subsampling.</em><br />
<a href="files/report_3.pdf">report</a> |
<a href="files/presentation_4.pdf">slides</a></li>
  <li><em>Statistical learning under a non-iid data generating process.</em><br />
<a href="files/report_2.pdf">report</a></li>
</ul>
:ET