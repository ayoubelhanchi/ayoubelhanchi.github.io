---
layout: page
---
<img style="float: right;
    margin-top: 25px;
    margin-bottom: 35px;
    max-width: 30%;
    border: 6px solid #ddd;
    border-radius: 50%;
    box-sizing: border-box;"
    src="files/picture_3.png">

I am a third year Ph.D. student in the [Department of Computer Science](https://web.cs.toronto.edu/) at the University of Toronto, supervised by [Chris Maddison](https://www.cs.toronto.edu/~cmaddis/) and [Murat Erdogdu](https://www.cs.toronto.edu/~erdogdu/). Previously, I did my M.Sc. degree in the [Department of Mathematics and Statistics](https://www.mcgill.ca/mathstat/) at McGill University under the supervision of [David Stephens](https://www.math.mcgill.ca/dstephens/). Before that, I did my B.Sc. degree in
[Honours Mathematics and Computer Science](https://www.mcgill.ca/mathstat/undergraduate/programs/b-sc/joint-honours-mathematics-and-computer-science-b-sc) at McGill University.


<!---[Resume](files/resume.pdf) / [Google Scholar](https://scholar.google.com/citations?user=5ZzcGmgAAAAJ&hl=en&oi=ao)-->


### Papers ###

[Minimax Linear Regression under the Quantile Risk.](files/paper_5.pdf)  
Ayoub El Hanchi, Chris J. Maddison, Murat A. Erdogdu.  
<!---*Conference on Learning Theory, 2024.*  -->
COLT 2024.  
<!---[paper](files/paper_5.pdf)-->

**Optimal Excess Risk Bounds for Empirical Risk Minimization on p-Norm Linear Regression.**  
Ayoub El Hanchi, Murat A. Erdogdu.  
<!---*Conference on Neural Information Processing Systems, 2023.*  -->
NeurIPS 2023.  
[paper](files/paper_4.pdf)

**Contrastive Learning Can Find an Optimal Basis for Approximately View-invariant Functions.**  
Daniel D. Johnson, Ayoub El Hanchi, Chris J. Maddison.  
<!---*International Conference on Learning Representations, 2023.*  -->
ICLR 2023.  
[paper](files/paper_3.pdf)

**Stochastic Reweighted Gradient Descent.**  
Ayoub El Hanchi, David A. Stephens, Chris J. Maddison.  
<!---*International Conference on Machine Learning, 2022.*  -->
ICML 2022.  
[paper](files/paper_2.pdf)

**Adaptive Importance Sampling for Finite-Sum Optimization and Sampling with Decreasing Step-Sizes.**  
Ayoub El Hanchi, David A. Stephens.  
<!---*Conference on Neural Information Processing Systems, 2020.*  -->
NeurIPS 2020.  
[paper](files/paper_1.pdf) <!---| [slides](files/presentation_1.pdf) | [poster](files/poster_1.pdf)-->

<!---
### Notes ###



**A Lyapunov Analysis of Loopless SARAH.**  
Ayoub El Hanchi  
[paper](files/paper_2.pdf)

### Thesis ###
**Large Scale Optimization and Sampling for Machine Learning and Statistics.**  
M.Sc. in Mathematics and Statistics, McGill University, May 2021.  
[thesis](files/thesis_1.pdf)
-->

<!---
### Software ###
**TorchVr (in progress)**  
A PyTorch library providing PyTorch modules and samplers that produce efficient gradient estimators to accelerate training of large scale models.  
All samplers are written in C++ using an efficient tree implementation for increased performance.
The C++ code is then exposed to python
using [pybind11](https://github.com/pybind/pybind11).  
<a href="files/code_1.zip">source

### Old Reports  ###
+ *Langevin Diffusion as Gradient Flow in Wasserstein Space.*  
<a href="files/report_4.pdf">report</a>
+ *Scaling up MCMC for Bayesian inference using adaptive data subsampling.*  
<a href="files/report_3.pdf">report</a> \|
<a href="files/presentation_4.pdf">slides</a>
+ *Statistical learning under a non-iid data generating process.*  
<a href="files/report_2.pdf">report</a>
-->